import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
import os
import warnings
warnings.filterwarnings('ignore')

def load_zomato_data(file_path):
    """Load and validate the Zomato dataset"""
    try:
        # Check if file exists
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"The file '{file_path}' does not exist.")
        
        # Check file size
        file_size = os.path.getsize(file_path)
        if file_size == 0:
            raise ValueError("The file is empty.")
        
        # Try different encodings
        encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252']
        
        for encoding in encodings:
            try:
                print(f"Trying encoding: {encoding}")
                df = pd.read_csv(file_path, encoding=encoding)
                print(f"Successfully loaded data with {encoding} encoding")
                print(f"Dataset shape: {df.shape}")
                return df
            except UnicodeDecodeError:
                continue
            except Exception as e:
                print(f"Error with {encoding} encoding: {str(e)}")
                continue
        
        raise ValueError("Failed to read the file with any encoding.")
        
    except Exception as e:
        print(f"Error loading CSV file: {str(e)}")
        print("Please check if the CSV file is complete and correctly formatted.")
        return None

def clean_zomato_data(df):
    """Clean and preprocess the Zomato dataset"""
    # Create a copy to avoid SettingWithCopyWarning
    df_clean = df.copy()
    
    # Display initial info
    print("\nInitial dataset info:")
    print(f"Shape: {df_clean.shape}")
    print("\nColumns:")
    print(df_clean.columns.tolist())
    print("\nMissing values:")
    print(df_clean.isnull().sum())
    
    # Clean the rating column
    if 'rate' in df_clean.columns:
        print("\nCleaning 'rate' column...")
        # Remove rows with missing ratings
        df_clean = df_clean.dropna(subset=['rate'])
        # Convert rating to numeric (e.g., "4.1/5" → 4.1)
        df_clean['rating_numeric'] = df_clean['rate'].astype(str).str.split('/').str[0].str.strip()
        df_clean['rating_numeric'] = pd.to_numeric(df_clean['rating_numeric'], errors='coerce')
        df_clean = df_clean.dropna(subset=['rating_numeric'])
    else:
        print("'rate' column not found. Creating a dummy rating column.")
        df_clean['rating_numeric'] = np.random.uniform(3, 5, len(df_clean))
    
    # Clean cost column
    if 'approx_cost(for two people)' in df_clean.columns:
        print("Cleaning 'approx_cost(for two people)' column...")
        df_clean['cost_for_two'] = df_clean['approx_cost(for two people)'].astype(str)
        df_clean['cost_for_two'] = df_clean['cost_for_two'].str.replace(',', '')
        df_clean['cost_for_two'] = pd.to_numeric(df_clean['cost_for_two'], errors='coerce')
        df_clean['cost_for_two'] = df_clean['cost_for_two'].fillna(df_clean['cost_for_two'].median())
    else:
        print("'approx_cost(for two people)' column not found. Creating a dummy cost column.")
        df_clean['cost_for_two'] = np.random.randint(300, 1500, len(df_clean))
    
    # Clean votes column
    if 'votes' in df_clean.columns:
        df_clean['votes'] = pd.to_numeric(df_clean['votes'], errors='coerce')
        df_clean['votes'] = df_clean['votes'].fillna(0)
    else:
        print("'votes' column not found. Creating a dummy votes column.")
        df_clean['votes'] = np.random.randint(0, 1000, len(df_clean))
    
    # Handle binary features
    if 'online_order' in df_clean.columns:
        df_clean['online_order'] = df_clean['online_order'].map({'Yes': 1, 'No': 0})
        df_clean['online_order'] = df_clean['online_order'].fillna(0)
    else:
        print("'online_order' column not found. Creating a dummy online_order column.")
        df_clean['online_order'] = np.random.randint(0, 2, len(df_clean))
    
    if 'book_table' in df_clean.columns:
        df_clean['book_table'] = df_clean['book_table'].map({'Yes': 1, 'No': 0})
        df_clean['book_table'] = df_clean['book_table'].fillna(0)
    else:
        print("'book_table' column not found. Creating a dummy book_table column.")
        df_clean['book_table'] = np.random.randint(0, 2, len(df_clean))
    
    # Fill missing values for categorical features
    categorical_cols = ['location', 'rest_type', 'cuisines']
    for col in categorical_cols:
        if col in df_clean.columns:
            df_clean[col] = df_clean[col].fillna('Unknown')
        else:
            print(f"'{col}' column not found. Creating a dummy {col} column.")
            df_clean[col] = f"Unknown_{col}"
    
    # Create target variable for classification
    df_clean['high_rating'] = (df_clean['rating_numeric'] >= 4.0).astype(int)
    
    print(f"\nCleaned dataset shape: {df_clean.shape}")
    return df_clean

def train_model(df_clean):
    """Train a machine learning model on the cleaned data"""
    print("\n=== MODEL TRAINING ===")
    
    # Select features and target
    categorical_features = ['location', 'rest_type', 'cuisines']
    numeric_features = ['online_order', 'book_table', 'cost_for_two', 'votes']
    
    # Ensure all required columns exist
    for col in categorical_features + numeric_features:
        if col not in df_clean.columns:
            print(f"Warning: '{col}' not found in dataset. Using alternative columns.")
            # If this happens, we might need to adjust our feature selection
    
    X = df_clean[categorical_features + numeric_features]
    y = df_clean['high_rating']
    
    # Check if we have enough data
    if len(X) < 100:
        print("Warning: Very small dataset. Results may not be reliable.")
    
    # Split the data
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, stratify=y, random_state=42
    )
    
    print(f"Training set size: {X_train.shape[0]}")
    print(f"Test set size: {X_test.shape[0]}")
    
    # Create preprocessing pipeline
    preprocessor = ColumnTransformer(
        transformers=[
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
            ('num', StandardScaler(), numeric_features)
        ])
    
    # Create and train the model
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(
            n_estimators=100, 
            random_state=42, 
            class_weight='balanced',
            max_depth=10
        ))
    ])
    
    print("Training model...")
    pipeline.fit(X_train, y_train)
    
    # Make predictions
    y_pred = pipeline.predict(X_test)
    y_proba = pipeline.predict_proba(X_test)[:, 1]
    
    # Evaluate the model
    print("\n=== MODEL EVALUATION ===")
    print("Accuracy:", round(accuracy_score(y_test, y_pred), 3))
    print("ROC AUC:", round(roc_auc_score(y_test, y_proba), 3))
    print("Baseline accuracy (predicting majority class):", round(max(y_test.mean(), 1 - y_test.mean()), 3))
    
    return pipeline, X_test, y_test

def main():
    """Main function to run the complete analysis"""
    print("=== ZOMATO BANGALORE RESTAURANT ANALYSIS ===")
    
    # Try to find the CSV file with different possible names
    possible_filenames = [
        'zomato.csv',
        'zomato_bangalore.csv',
        'zomato_bangalore_restaurants.csv',
        'dataset.csv'
    ]
    
    df = None
    for filename in possible_filenames:
        if os.path.exists(filename):
            print(f"Found file: {filename}")
            df = load_zomato_data(filename)
            if df is not None:
                break
    
    if df is None:
        print("Could not find a valid CSV file. Please make sure the file exists.")
        print("Tried filenames:", possible_filenames)
        return
    
    # Clean the data
    df_clean = clean_zomato_data(df)
    
    # Train the model
    pipeline, X_test, y_test = train_model(df_clean)
    
    # Display additional insights
    print("\n=== DATASET INSIGHTS ===")
    print(f"Total restaurants: {len(df_clean)}")
    print(f"High rating restaurants (≥4.0): {df_clean['high_rating'].sum()} ({df_clean['high_rating'].mean()*100:.1f}%)")
    
    if 'rating_numeric' in df_clean.columns:
        print(f"Average rating: {df_clean['rating_numeric'].mean():.2f}")
    
    if 'cost_for_two' in df_clean.columns:
        print(f"Average cost for two: ₹{df_clean['cost_for_two'].mean():.2f}")
    
    # Sample prediction
    print("\n=== SAMPLE PREDICTION ===")
    sample_restaurant = pd.DataFrame({
        'location': ['BTM'],
        'rest_type': ['Casual Dining'],
        'cuisines': ['North Indian, Chinese'],
        'online_order': [1],
        'book_table': [1],
        'cost_for_two': [800],
        'votes': [1000]
    })
    
    # Ensure sample has all required columns
    for col in ['location', 'rest_type', 'cuisines', 'online_order', 'book_table', 'cost_for_two', 'votes']:
        if col not in sample_restaurant.columns:
            sample_restaurant[col] = 0  # or appropriate default
    
    try:
        prediction = pipeline.predict(sample_restaurant)
        probability = pipeline.predict_proba(sample_restaurant)[:, 1]
        
        print("Sample restaurant features:")
        for col, value in sample_restaurant.items():
            print(f"  {col}: {value[0]}")
        print(f"Prediction: {'High Rating (≥4.0)' if prediction[0] == 1 else 'Low Rating (<4.0)'}")
        print(f"Probability: {probability[0]:.3f}")
    except Exception as e:
        print(f"Error making sample prediction: {str(e)}")

if __name__ == "__main__":
    main()